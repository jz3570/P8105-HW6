---
title: "P8105"
author: "Jiawen Zhao"
date: "11/25/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(patchwork)
library(gridExtra)
library(modelr)
```

#### Problem 2

```{r}
homicide <- read.csv("./data/homicide-data.csv")%>% 
  na.omit() %>% 
  unite(city_state, c(city,state),sep = ", ") %>% 
  mutate(solved = case_when(grepl("Closed by arrest", disposition)==TRUE ~ 1,
  grepl("Closed by arrest", disposition)==FALSE ~ 0)) %>% 
  mutate(unsolved = case_when(grepl("Closed by arrest", disposition)==FALSE ~ 1,
  grepl("Closed by arrest", disposition)==TRUE ~ 0)) %>% 
  filter(!str_detect(city_state, "Dallas, TX"))%>% 
  filter(!str_detect(city_state, "Phoenix, AZ"))%>% 
  filter(!str_detect(city_state, "Kansas City, MO"))%>% 
  filter(!str_detect(city_state, "Tulsa, AL"))%>% 
  filter(str_detect(victim_race, c("White", "Black"))) %>% 
  mutate(victim_age =as.numeric(victim_age))%>% 
  na.omit()
BMD = filter(homicide,city_state=="Baltimore, MD")
fit_logistic = BMD%>% 
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
fit_logistic %>% 
  broom::tidy(conf.int = TRUE) %>% 
  mutate(OR = exp(estimate), adj.conf.low=exp(conf.low), adj.conf.high=exp(conf.high)) %>%
  select(term, log_OR = estimate, OR, p.value, adj.conf.low, adj.conf.high, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)
exp(summary(fit_logistic)$coefficients["victim_sexMale",1] + qnorm(c(0.025,0.5,0.975)) * summary(fit_logistic)$coefficients["victim_sexMale",2])
```

Keeping all other variables fixed, the estimate of the adjusted odds ratio for solving homicides comparing male victims to female victims is 0.4687305, with confidence interval of (0.3193133, 0.6880650).

```{r}
city_glm = 
  homicide %>%
  nest(data = -city_state) %>% 
  mutate(model = map(data, ~glm(solved~victim_age + victim_race + victim_sex, data = ., family = binomial())),
         result = map(.x=model, ~broom::tidy(.x,conf.int = TRUE))) %>% 
  select( city_state,result)%>% 
  unnest(result)%>% 
  mutate(OR = exp(estimate), adj.conf.low=exp(conf.low), adj.conf.high=exp(conf.high)) %>%
  select(term, log_OR = estimate, OR, p.value, adj.conf.low, adj.conf.high) 

```


# Problem 3

```{r}
birthweight <- read.csv("./data/birthweight.csv")%>% 
  na.omit() %>% 
  mutate(babysex = as.factor(babysex)) %>% 
  mutate(frace = as.factor(frace))%>% 
  mutate(mrace = as.factor(mrace))%>% 
  mutate(malform = as.factor(malform))
  
birthweight %>% 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5)

fit1 = lm(bwt ~ bhead, data = birthweight)
summary(fit1)
summary(fit1)$coef
coef(fit1)
#fitted.values(fit1)
fit1 %>% 
  broom::glance()%>% 
  broom::tidy()

birthweight %>% 
  modelr::add_residuals(fit1) %>% 
  modelr::add_predictions(fit1) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point()

#plot(fit1)
```
  

```{r}
fit2 = lm(bwt ~ blength + gaweeks, data = birthweight)
summary(fit2)
summary(fit2)$coef
coef(fit2)
#fitted.values(fit2)
fit2 %>% 
  broom::glance()%>% 
  broom::tidy()

birthweight %>% 
  modelr::add_residuals(fit2) %>% 
  modelr::add_predictions(fit2) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point()

```

```{r}
fit3 = glm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = birthweight)
summary(fit3)
summary(fit3)$coef
coef(fit3)
#fitted.values(fit3)
fit3 %>% 
  broom::glance()%>% 
  broom::tidy()

birthweight %>% 
  modelr::add_residuals(fit3) %>% 
  modelr::add_predictions(fit3) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point()

```

```{r}
cv_df =
  crossv_mc(birthweight, 100) %>% ##always use 100??????
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
# cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
# cv_df %>% pull(test) %>% .[[1]] %>% as_tibble
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    fit1  = map(train, ~lm(bwt ~ blength, data = .x)),
    fit2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit3  = map(train, ~glm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_fit1 = map2_dbl(fit1, test, ~rmse(model = .x, data = .y)),
    rmse_fit2 = map2_dbl(fit2, test, ~rmse(model = .x, data = .y)),
    rmse_fit3 = map2_dbl(fit3, test, ~rmse(model = .x, data = .y)))

```


```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

```


We can see that the last model, general linear model, is better than the simple linear models. The first model with only one predictor is the worst, and as we add more predictors and more appropriate predictors, the model gets improved.

